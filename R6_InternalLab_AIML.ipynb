{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "R6_InternalLab_AIML.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "c7SZXKzCS4qu",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from google.colab import drive"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "znJ_4CdoyIy8",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "8rU-XgfCUlKI",
        "colab_type": "code",
        "outputId": "1a2c753f-9bd4-4aa0-abf6-67dbc564b2d4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "cell_type": "code",
      "source": [
        "drive.mount(\"/content/gdrive\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "zdvbh0B0V1AS",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#content/gdrive/My Drive/AIML/INTERNAL RESIDENCY6\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "rHzZiIpQV7dj",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import tensorflow as tf"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "k5M-mo35WBce",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#Reset Default graph - Needed only for Jupyter notebook\n",
        "tf.reset_default_graph()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "qJ9cyN_5W_po",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**collect** data"
      ]
    },
    {
      "metadata": {
        "id": "y66l6Of7XHFW",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import pandas as pd\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "m8AgyqcWY4d6",
        "colab_type": "code",
        "outputId": "c48fecc0-8ea9-4ebc-ce5c-69eb17de923b",
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 106
        }
      },
      "cell_type": "code",
      "source": [
        "#!ls \"/content/gdrive/My Drive/AIML/INTERNAL RESIDENCY6\"\n",
        "#data=pd.read_csv(\"content/gdrive/My Drive/AIML/INTERNAL RESIDENCY6/Iris.csv\")\n",
        "from google.colab import files\n",
        "uploaded = files.upload()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-931ecfad-0b3a-49dc-b13b-60c6f56d8210\" name=\"files[]\" multiple disabled />\n",
              "     <output id=\"result-931ecfad-0b3a-49dc-b13b-60c6f56d8210\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saving Iris.csv to Iris.csv\n",
            "Saving prices.csv to prices.csv\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "B3aOC7VWhMma",
        "colab_type": "code",
        "outputId": "65eaadf9-d866-4239-b476-e0ae216c089d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "cell_type": "code",
      "source": [
        "import io\n",
        "df = pd.read_csv(io.StringIO(uploaded['prices.csv'].decode('utf-8')))\n",
        "df.head()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>date</th>\n",
              "      <th>symbol</th>\n",
              "      <th>open</th>\n",
              "      <th>close</th>\n",
              "      <th>low</th>\n",
              "      <th>high</th>\n",
              "      <th>volume</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2016-01-05 00:00:00</td>\n",
              "      <td>WLTW</td>\n",
              "      <td>123.430000</td>\n",
              "      <td>125.839996</td>\n",
              "      <td>122.309998</td>\n",
              "      <td>126.250000</td>\n",
              "      <td>2163600.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2016-01-06 00:00:00</td>\n",
              "      <td>WLTW</td>\n",
              "      <td>125.239998</td>\n",
              "      <td>119.980003</td>\n",
              "      <td>119.940002</td>\n",
              "      <td>125.540001</td>\n",
              "      <td>2386400.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2016-01-07 00:00:00</td>\n",
              "      <td>WLTW</td>\n",
              "      <td>116.379997</td>\n",
              "      <td>114.949997</td>\n",
              "      <td>114.930000</td>\n",
              "      <td>119.739998</td>\n",
              "      <td>2489500.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2016-01-08 00:00:00</td>\n",
              "      <td>WLTW</td>\n",
              "      <td>115.480003</td>\n",
              "      <td>116.620003</td>\n",
              "      <td>113.500000</td>\n",
              "      <td>117.440002</td>\n",
              "      <td>2006300.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2016-01-11 00:00:00</td>\n",
              "      <td>WLTW</td>\n",
              "      <td>117.010002</td>\n",
              "      <td>114.970001</td>\n",
              "      <td>114.089996</td>\n",
              "      <td>117.330002</td>\n",
              "      <td>1408600.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                  date symbol        open       close         low        high  \\\n",
              "0  2016-01-05 00:00:00   WLTW  123.430000  125.839996  122.309998  126.250000   \n",
              "1  2016-01-06 00:00:00   WLTW  125.239998  119.980003  119.940002  125.540001   \n",
              "2  2016-01-07 00:00:00   WLTW  116.379997  114.949997  114.930000  119.739998   \n",
              "3  2016-01-08 00:00:00   WLTW  115.480003  116.620003  113.500000  117.440002   \n",
              "4  2016-01-11 00:00:00   WLTW  117.010002  114.970001  114.089996  117.330002   \n",
              "\n",
              "      volume  \n",
              "0  2163600.0  \n",
              "1  2386400.0  \n",
              "2  2489500.0  \n",
              "3  2006300.0  \n",
              "4  1408600.0  "
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 79
        }
      ]
    },
    {
      "metadata": {
        "id": "4ALYqev3hgb0",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "f1TE8E6mhmV9",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Check all columns in the dataset"
      ]
    },
    {
      "metadata": {
        "id": "pkdBHcqehndC",
        "colab_type": "code",
        "outputId": "bdfd0b93-06aa-467d-da52-e5f4e4466718",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        }
      },
      "cell_type": "code",
      "source": [
        "df.info()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 851264 entries, 0 to 851263\n",
            "Data columns (total 7 columns):\n",
            "date      851264 non-null object\n",
            "symbol    851264 non-null object\n",
            "open      851264 non-null float64\n",
            "close     851264 non-null float64\n",
            "low       851264 non-null float64\n",
            "high      851264 non-null float64\n",
            "volume    851264 non-null float64\n",
            "dtypes: float64(5), object(2)\n",
            "memory usage: 45.5+ MB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "wf809RyThwyF",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Drop columns date and symbol"
      ]
    },
    {
      "metadata": {
        "id": "qEBCFp-zhycI",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "df.drop(df.columns[[0, 1]], axis=1, inplace=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "SOMideaFimk0",
        "colab_type": "code",
        "outputId": "9c69e5e3-b3cc-4c1a-d775-4296720ff3e1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "cell_type": "code",
      "source": [
        "df.head()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>open</th>\n",
              "      <th>close</th>\n",
              "      <th>low</th>\n",
              "      <th>high</th>\n",
              "      <th>volume</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>123.430000</td>\n",
              "      <td>125.839996</td>\n",
              "      <td>122.309998</td>\n",
              "      <td>126.250000</td>\n",
              "      <td>2163600.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>125.239998</td>\n",
              "      <td>119.980003</td>\n",
              "      <td>119.940002</td>\n",
              "      <td>125.540001</td>\n",
              "      <td>2386400.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>116.379997</td>\n",
              "      <td>114.949997</td>\n",
              "      <td>114.930000</td>\n",
              "      <td>119.739998</td>\n",
              "      <td>2489500.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>115.480003</td>\n",
              "      <td>116.620003</td>\n",
              "      <td>113.500000</td>\n",
              "      <td>117.440002</td>\n",
              "      <td>2006300.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>117.010002</td>\n",
              "      <td>114.970001</td>\n",
              "      <td>114.089996</td>\n",
              "      <td>117.330002</td>\n",
              "      <td>1408600.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "         open       close         low        high     volume\n",
              "0  123.430000  125.839996  122.309998  126.250000  2163600.0\n",
              "1  125.239998  119.980003  119.940002  125.540001  2386400.0\n",
              "2  116.379997  114.949997  114.930000  119.739998  2489500.0\n",
              "3  115.480003  116.620003  113.500000  117.440002  2006300.0\n",
              "4  117.010002  114.970001  114.089996  117.330002  1408600.0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 82
        }
      ]
    },
    {
      "metadata": {
        "id": "JUMveCi_i-p4",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Consider only first 1000 rows in the dataset for building feature set and target set"
      ]
    },
    {
      "metadata": {
        "id": "vKtwPPNPjCpV",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "df2 = df.head(1000)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "BVwlYYH0jYTN",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Divide the data into train and test sets"
      ]
    },
    {
      "metadata": {
        "id": "6RmTSLGWjZ3O",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "y= np.arange(1000)\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(df2, y, test_size=0.2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "sQYoQ7wJmMzW",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Building the graph in tensorflow"
      ]
    },
    {
      "metadata": {
        "id": "vGBohjqZmRiK",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#Input features\n",
        "x = tf.placeholder(shape=[None,5],dtype=tf.float32, name='x-input')\n",
        "\n",
        "#Normalize the data\n",
        "x_n = tf.layers.batch_normalization(x,training=True)\n",
        "\n",
        "#Actual Prices\n",
        "y_ = tf.placeholder(shape=[None],dtype=tf.float32, name='y-input')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "XWcVfEuFpuXS",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#Define Weights and Bias\n",
        "W = tf.Variable(tf.zeros(shape=[5,1]), name=\"Weights\")\n",
        "b = tf.Variable(tf.zeros(shape=[1]),name=\"Bias\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ibDHacaTp6lG",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#prediction\n",
        "y = tf.add(tf.matmul(x_n,W),b,name='output')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "4GEp_Z1iqZzH",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#Loss (Cost) Function [Mean square error]\n",
        "loss = tf.reduce_mean(tf.square(y-y_),name='Loss')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "4_zwx4XMqmlO",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#GradientDescent Optimizer to minimize Loss\n",
        "train_op = tf.train.GradientDescentOptimizer(0.003).minimize(loss)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "YGLH821xtNQc",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Executing the Graph\n"
      ]
    },
    {
      "metadata": {
        "id": "G4wyBd3atOe6",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#Lets start graph Execution\n",
        "sess = tf.Session()\n",
        "\n",
        "# variables need to be initialized before we can use them\n",
        "sess.run(tf.global_variables_initializer())\n",
        "\n",
        "#how many times data need to be shown to model\n",
        "training_epochs = 100"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "h5kf209kuBpS",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#Load data\n",
        "\n",
        "#(features, actual_prices),_ = df2.load_data(test_split=0)\n",
        "\n",
        "features=df2\n",
        "actual_prices=np.arange(1000)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "WRqS-OsltvAP",
        "colab_type": "code",
        "outputId": "dfb097cc-6dae-4c34-9fd3-12d1a05b29cc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1717
        }
      },
      "cell_type": "code",
      "source": [
        "for epoch in range(1000):\n",
        "            \n",
        "    #Calculate train_op and loss\n",
        "    _, train_loss = sess.run([train_op,loss],feed_dict={x:features, \n",
        "                                                        y_:actual_prices})\n",
        "    \n",
        "    if epoch % 10 == 0:\n",
        "        print ('Training loss at step: ', epoch, ' is ', train_loss)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training loss at step:  0  is  332833.53\n",
            "Training loss at step:  10  is  304535.16\n",
            "Training loss at step:  20  is  nan\n",
            "Training loss at step:  30  is  nan\n",
            "Training loss at step:  40  is  nan\n",
            "Training loss at step:  50  is  nan\n",
            "Training loss at step:  60  is  nan\n",
            "Training loss at step:  70  is  nan\n",
            "Training loss at step:  80  is  nan\n",
            "Training loss at step:  90  is  nan\n",
            "Training loss at step:  100  is  nan\n",
            "Training loss at step:  110  is  nan\n",
            "Training loss at step:  120  is  nan\n",
            "Training loss at step:  130  is  nan\n",
            "Training loss at step:  140  is  nan\n",
            "Training loss at step:  150  is  nan\n",
            "Training loss at step:  160  is  nan\n",
            "Training loss at step:  170  is  nan\n",
            "Training loss at step:  180  is  nan\n",
            "Training loss at step:  190  is  nan\n",
            "Training loss at step:  200  is  nan\n",
            "Training loss at step:  210  is  nan\n",
            "Training loss at step:  220  is  nan\n",
            "Training loss at step:  230  is  nan\n",
            "Training loss at step:  240  is  nan\n",
            "Training loss at step:  250  is  nan\n",
            "Training loss at step:  260  is  nan\n",
            "Training loss at step:  270  is  nan\n",
            "Training loss at step:  280  is  nan\n",
            "Training loss at step:  290  is  nan\n",
            "Training loss at step:  300  is  nan\n",
            "Training loss at step:  310  is  nan\n",
            "Training loss at step:  320  is  nan\n",
            "Training loss at step:  330  is  nan\n",
            "Training loss at step:  340  is  nan\n",
            "Training loss at step:  350  is  nan\n",
            "Training loss at step:  360  is  nan\n",
            "Training loss at step:  370  is  nan\n",
            "Training loss at step:  380  is  nan\n",
            "Training loss at step:  390  is  nan\n",
            "Training loss at step:  400  is  nan\n",
            "Training loss at step:  410  is  nan\n",
            "Training loss at step:  420  is  nan\n",
            "Training loss at step:  430  is  nan\n",
            "Training loss at step:  440  is  nan\n",
            "Training loss at step:  450  is  nan\n",
            "Training loss at step:  460  is  nan\n",
            "Training loss at step:  470  is  nan\n",
            "Training loss at step:  480  is  nan\n",
            "Training loss at step:  490  is  nan\n",
            "Training loss at step:  500  is  nan\n",
            "Training loss at step:  510  is  nan\n",
            "Training loss at step:  520  is  nan\n",
            "Training loss at step:  530  is  nan\n",
            "Training loss at step:  540  is  nan\n",
            "Training loss at step:  550  is  nan\n",
            "Training loss at step:  560  is  nan\n",
            "Training loss at step:  570  is  nan\n",
            "Training loss at step:  580  is  nan\n",
            "Training loss at step:  590  is  nan\n",
            "Training loss at step:  600  is  nan\n",
            "Training loss at step:  610  is  nan\n",
            "Training loss at step:  620  is  nan\n",
            "Training loss at step:  630  is  nan\n",
            "Training loss at step:  640  is  nan\n",
            "Training loss at step:  650  is  nan\n",
            "Training loss at step:  660  is  nan\n",
            "Training loss at step:  670  is  nan\n",
            "Training loss at step:  680  is  nan\n",
            "Training loss at step:  690  is  nan\n",
            "Training loss at step:  700  is  nan\n",
            "Training loss at step:  710  is  nan\n",
            "Training loss at step:  720  is  nan\n",
            "Training loss at step:  730  is  nan\n",
            "Training loss at step:  740  is  nan\n",
            "Training loss at step:  750  is  nan\n",
            "Training loss at step:  760  is  nan\n",
            "Training loss at step:  770  is  nan\n",
            "Training loss at step:  780  is  nan\n",
            "Training loss at step:  790  is  nan\n",
            "Training loss at step:  800  is  nan\n",
            "Training loss at step:  810  is  nan\n",
            "Training loss at step:  820  is  nan\n",
            "Training loss at step:  830  is  nan\n",
            "Training loss at step:  840  is  nan\n",
            "Training loss at step:  850  is  nan\n",
            "Training loss at step:  860  is  nan\n",
            "Training loss at step:  870  is  nan\n",
            "Training loss at step:  880  is  nan\n",
            "Training loss at step:  890  is  nan\n",
            "Training loss at step:  900  is  nan\n",
            "Training loss at step:  910  is  nan\n",
            "Training loss at step:  920  is  nan\n",
            "Training loss at step:  930  is  nan\n",
            "Training loss at step:  940  is  nan\n",
            "Training loss at step:  950  is  nan\n",
            "Training loss at step:  960  is  nan\n",
            "Training loss at step:  970  is  nan\n",
            "Training loss at step:  980  is  nan\n",
            "Training loss at step:  990  is  nan\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "VdVcrT2syL-k",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "ebb52f9a-5782-4629-e44d-5e2aeb278ca8"
      },
      "cell_type": "code",
      "source": [
        "sess.run(y, feed_dict={x:features[0:1]})"
      ],
      "execution_count": 93,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[nan]], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 93
        }
      ]
    },
    {
      "metadata": {
        "id": "Vf1V3cD0yXTs",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "25de74cf-40b8-421d-ee01-5f996cbacc82"
      },
      "cell_type": "code",
      "source": [
        "sess.run(y, feed_dict={x:[[1,2,3,400,5],\n",
        "                          [1,2,3,400,5]]})"
      ],
      "execution_count": 97,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[nan],\n",
              "       [nan]], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 97
        }
      ]
    },
    {
      "metadata": {
        "id": "9ZbUNIlhyuc0",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "sD3CIbE5y15n",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Linear Classification using Keras\n",
        "\n",
        "Building the simple Neural Network in Keras with one neuron in the dense hidden layer.\n",
        "Use Mean square error as loss function and sgd as optimize"
      ]
    },
    {
      "metadata": {
        "id": "fWNrkJKCy3Yh",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "3763efcd-cb86-45a0-abd2-d518d3d29fe8"
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "from keras.utils import to_categorical"
      ],
      "execution_count": 98,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "HnFNLbhb0l8C",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "4b8e4a7c-8075-47fe-d6f8-702c004e4b7a"
      },
      "cell_type": "code",
      "source": [
        "dataset = pd.read_csv(io.StringIO(uploaded['Iris.csv'].decode('utf-8')))\n",
        "dataset.head()"
      ],
      "execution_count": 100,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Id</th>\n",
              "      <th>SepalLengthCm</th>\n",
              "      <th>SepalWidthCm</th>\n",
              "      <th>PetalLengthCm</th>\n",
              "      <th>PetalWidthCm</th>\n",
              "      <th>Species</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>5.1</td>\n",
              "      <td>3.5</td>\n",
              "      <td>1.4</td>\n",
              "      <td>0.2</td>\n",
              "      <td>Iris-setosa</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>4.9</td>\n",
              "      <td>3.0</td>\n",
              "      <td>1.4</td>\n",
              "      <td>0.2</td>\n",
              "      <td>Iris-setosa</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>4.7</td>\n",
              "      <td>3.2</td>\n",
              "      <td>1.3</td>\n",
              "      <td>0.2</td>\n",
              "      <td>Iris-setosa</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4</td>\n",
              "      <td>4.6</td>\n",
              "      <td>3.1</td>\n",
              "      <td>1.5</td>\n",
              "      <td>0.2</td>\n",
              "      <td>Iris-setosa</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5</td>\n",
              "      <td>5.0</td>\n",
              "      <td>3.6</td>\n",
              "      <td>1.4</td>\n",
              "      <td>0.2</td>\n",
              "      <td>Iris-setosa</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Id  SepalLengthCm  SepalWidthCm  PetalLengthCm  PetalWidthCm      Species\n",
              "0   1            5.1           3.5            1.4           0.2  Iris-setosa\n",
              "1   2            4.9           3.0            1.4           0.2  Iris-setosa\n",
              "2   3            4.7           3.2            1.3           0.2  Iris-setosa\n",
              "3   4            4.6           3.1            1.5           0.2  Iris-setosa\n",
              "4   5            5.0           3.6            1.4           0.2  Iris-setosa"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 100
        }
      ]
    },
    {
      "metadata": {
        "id": "0MpjT_SY0s1n",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#Splitting the data into training and test test\n",
        "X = dataset.iloc[:,1:5].values\n",
        "y = dataset.iloc[:,5].values\n",
        "\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "encoder =  LabelEncoder()\n",
        "y1 = encoder.fit_transform(y)\n",
        "\n",
        "Y = pd.get_dummies(y1).values\n",
        "\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train,X_test, y_train,y_test = train_test_split(X,Y,test_size=0.2,random_state=0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "eSRnCaK908zT",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 272
        },
        "outputId": "6a69978b-9240-4e54-f3fa-9dab1d4f4aa9"
      },
      "cell_type": "code",
      "source": [
        "#Defining the model \n",
        "\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.optimizers import SGD,Adam\n",
        "\n",
        "\n",
        "model = Sequential()\n",
        "\n",
        "model.add(Dense(10,input_shape=(4,),activation='tanh'))\n",
        "model.add(Dense(8,activation='tanh'))\n",
        "model.add(Dense(6,activation='tanh'))\n",
        "model.add(Dense(3,activation='softmax'))\n",
        "\n",
        "model.compile(Adam(lr=0.04),'categorical_crossentropy',metrics=['accuracy'])\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": 102,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_1 (Dense)              (None, 10)                50        \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 8)                 88        \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 6)                 54        \n",
            "_________________________________________________________________\n",
            "dense_4 (Dense)              (None, 3)                 21        \n",
            "=================================================================\n",
            "Total params: 213\n",
            "Trainable params: 213\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "xWXz6c841HUw",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 3417
        },
        "outputId": "32888cd9-0b52-46bb-d376-10c429233584"
      },
      "cell_type": "code",
      "source": [
        "#fitting the model and predicting \n",
        "model.fit(X_train,y_train,epochs=100)\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "y_test_class = np.argmax(y_test,axis=1)\n",
        "y_pred_class = np.argmax(y_pred,axis=1)"
      ],
      "execution_count": 103,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "120/120 [==============================] - 0s 3ms/step - loss: 1.1426 - acc: 0.3917\n",
            "Epoch 2/100\n",
            "120/120 [==============================] - 0s 206us/step - loss: 1.0968 - acc: 0.4333\n",
            "Epoch 3/100\n",
            "120/120 [==============================] - 0s 230us/step - loss: 0.9846 - acc: 0.5333\n",
            "Epoch 4/100\n",
            "120/120 [==============================] - 0s 225us/step - loss: 0.8049 - acc: 0.6917\n",
            "Epoch 5/100\n",
            "120/120 [==============================] - 0s 212us/step - loss: 0.6129 - acc: 0.7000\n",
            "Epoch 6/100\n",
            "120/120 [==============================] - 0s 213us/step - loss: 0.5004 - acc: 0.7000\n",
            "Epoch 7/100\n",
            "120/120 [==============================] - 0s 204us/step - loss: 0.4471 - acc: 0.9417\n",
            "Epoch 8/100\n",
            "120/120 [==============================] - 0s 225us/step - loss: 0.4178 - acc: 0.8000\n",
            "Epoch 9/100\n",
            "120/120 [==============================] - 0s 228us/step - loss: 0.3895 - acc: 0.8000\n",
            "Epoch 10/100\n",
            "120/120 [==============================] - 0s 240us/step - loss: 0.2962 - acc: 0.8917\n",
            "Epoch 11/100\n",
            "120/120 [==============================] - 0s 228us/step - loss: 0.2787 - acc: 0.8833\n",
            "Epoch 12/100\n",
            "120/120 [==============================] - 0s 242us/step - loss: 0.2163 - acc: 0.9417\n",
            "Epoch 13/100\n",
            "120/120 [==============================] - 0s 229us/step - loss: 0.2126 - acc: 0.9417\n",
            "Epoch 14/100\n",
            "120/120 [==============================] - 0s 217us/step - loss: 0.1367 - acc: 0.9417\n",
            "Epoch 15/100\n",
            "120/120 [==============================] - 0s 218us/step - loss: 0.2370 - acc: 0.9167\n",
            "Epoch 16/100\n",
            "120/120 [==============================] - 0s 251us/step - loss: 0.1386 - acc: 0.9417\n",
            "Epoch 17/100\n",
            "120/120 [==============================] - 0s 233us/step - loss: 0.1075 - acc: 0.9583\n",
            "Epoch 18/100\n",
            "120/120 [==============================] - 0s 304us/step - loss: 0.0925 - acc: 0.9667\n",
            "Epoch 19/100\n",
            "120/120 [==============================] - 0s 224us/step - loss: 0.1048 - acc: 0.9500\n",
            "Epoch 20/100\n",
            "120/120 [==============================] - 0s 205us/step - loss: 0.1276 - acc: 0.9583\n",
            "Epoch 21/100\n",
            "120/120 [==============================] - 0s 198us/step - loss: 0.1209 - acc: 0.9500\n",
            "Epoch 22/100\n",
            "120/120 [==============================] - 0s 223us/step - loss: 0.0794 - acc: 0.9667\n",
            "Epoch 23/100\n",
            "120/120 [==============================] - 0s 228us/step - loss: 0.1139 - acc: 0.9583\n",
            "Epoch 24/100\n",
            "120/120 [==============================] - 0s 217us/step - loss: 0.0870 - acc: 0.9667\n",
            "Epoch 25/100\n",
            "120/120 [==============================] - 0s 244us/step - loss: 0.1851 - acc: 0.9250\n",
            "Epoch 26/100\n",
            "120/120 [==============================] - 0s 257us/step - loss: 0.2976 - acc: 0.8833\n",
            "Epoch 27/100\n",
            "120/120 [==============================] - 0s 215us/step - loss: 0.2386 - acc: 0.9000\n",
            "Epoch 28/100\n",
            "120/120 [==============================] - 0s 209us/step - loss: 0.2677 - acc: 0.9000\n",
            "Epoch 29/100\n",
            "120/120 [==============================] - 0s 218us/step - loss: 0.1681 - acc: 0.9250\n",
            "Epoch 30/100\n",
            "120/120 [==============================] - 0s 221us/step - loss: 0.2028 - acc: 0.9167\n",
            "Epoch 31/100\n",
            "120/120 [==============================] - 0s 236us/step - loss: 0.1257 - acc: 0.9583\n",
            "Epoch 32/100\n",
            "120/120 [==============================] - 0s 240us/step - loss: 0.0998 - acc: 0.9833\n",
            "Epoch 33/100\n",
            "120/120 [==============================] - 0s 208us/step - loss: 0.1893 - acc: 0.9333\n",
            "Epoch 34/100\n",
            "120/120 [==============================] - 0s 187us/step - loss: 0.1291 - acc: 0.9500\n",
            "Epoch 35/100\n",
            "120/120 [==============================] - 0s 171us/step - loss: 0.1648 - acc: 0.9500\n",
            "Epoch 36/100\n",
            "120/120 [==============================] - 0s 212us/step - loss: 0.1191 - acc: 0.9583\n",
            "Epoch 37/100\n",
            "120/120 [==============================] - 0s 216us/step - loss: 0.1813 - acc: 0.9417\n",
            "Epoch 38/100\n",
            "120/120 [==============================] - 0s 207us/step - loss: 0.1578 - acc: 0.9417\n",
            "Epoch 39/100\n",
            "120/120 [==============================] - 0s 217us/step - loss: 0.1535 - acc: 0.9417\n",
            "Epoch 40/100\n",
            "120/120 [==============================] - 0s 241us/step - loss: 0.0955 - acc: 0.9750\n",
            "Epoch 41/100\n",
            "120/120 [==============================] - 0s 217us/step - loss: 0.0988 - acc: 0.9583\n",
            "Epoch 42/100\n",
            "120/120 [==============================] - 0s 214us/step - loss: 0.1004 - acc: 0.9500\n",
            "Epoch 43/100\n",
            "120/120 [==============================] - 0s 229us/step - loss: 0.1503 - acc: 0.9500\n",
            "Epoch 44/100\n",
            "120/120 [==============================] - 0s 234us/step - loss: 0.0698 - acc: 0.9750\n",
            "Epoch 45/100\n",
            "120/120 [==============================] - 0s 221us/step - loss: 0.1041 - acc: 0.9500\n",
            "Epoch 46/100\n",
            "120/120 [==============================] - 0s 242us/step - loss: 0.0698 - acc: 0.9750\n",
            "Epoch 47/100\n",
            "120/120 [==============================] - 0s 258us/step - loss: 0.0879 - acc: 0.9667\n",
            "Epoch 48/100\n",
            "120/120 [==============================] - 0s 228us/step - loss: 0.0843 - acc: 0.9583\n",
            "Epoch 49/100\n",
            "120/120 [==============================] - 0s 225us/step - loss: 0.0936 - acc: 0.9500\n",
            "Epoch 50/100\n",
            "120/120 [==============================] - 0s 246us/step - loss: 0.0756 - acc: 0.9667\n",
            "Epoch 51/100\n",
            "120/120 [==============================] - 0s 245us/step - loss: 0.0643 - acc: 0.9833\n",
            "Epoch 52/100\n",
            "120/120 [==============================] - 0s 213us/step - loss: 0.0609 - acc: 0.9833\n",
            "Epoch 53/100\n",
            "120/120 [==============================] - 0s 222us/step - loss: 0.0606 - acc: 0.9833\n",
            "Epoch 54/100\n",
            "120/120 [==============================] - 0s 286us/step - loss: 0.0673 - acc: 0.9833\n",
            "Epoch 55/100\n",
            "120/120 [==============================] - 0s 219us/step - loss: 0.0566 - acc: 0.9917\n",
            "Epoch 56/100\n",
            "120/120 [==============================] - 0s 223us/step - loss: 0.0808 - acc: 0.9667\n",
            "Epoch 57/100\n",
            "120/120 [==============================] - 0s 212us/step - loss: 0.1646 - acc: 0.9500\n",
            "Epoch 58/100\n",
            "120/120 [==============================] - 0s 248us/step - loss: 0.2008 - acc: 0.9167\n",
            "Epoch 59/100\n",
            "120/120 [==============================] - 0s 207us/step - loss: 0.2550 - acc: 0.8917\n",
            "Epoch 60/100\n",
            "120/120 [==============================] - 0s 225us/step - loss: 0.2246 - acc: 0.8750\n",
            "Epoch 61/100\n",
            "120/120 [==============================] - 0s 217us/step - loss: 0.1147 - acc: 0.9583\n",
            "Epoch 62/100\n",
            "120/120 [==============================] - 0s 236us/step - loss: 0.1699 - acc: 0.9167\n",
            "Epoch 63/100\n",
            "120/120 [==============================] - 0s 251us/step - loss: 0.1512 - acc: 0.9417\n",
            "Epoch 64/100\n",
            "120/120 [==============================] - 0s 214us/step - loss: 0.1475 - acc: 0.9500\n",
            "Epoch 65/100\n",
            "120/120 [==============================] - 0s 224us/step - loss: 0.0827 - acc: 0.9833\n",
            "Epoch 66/100\n",
            "120/120 [==============================] - 0s 249us/step - loss: 0.0780 - acc: 0.9667\n",
            "Epoch 67/100\n",
            "120/120 [==============================] - 0s 211us/step - loss: 0.0714 - acc: 0.9667\n",
            "Epoch 68/100\n",
            "120/120 [==============================] - 0s 249us/step - loss: 0.0715 - acc: 0.9667\n",
            "Epoch 69/100\n",
            "120/120 [==============================] - 0s 255us/step - loss: 0.0948 - acc: 0.9583\n",
            "Epoch 70/100\n",
            "120/120 [==============================] - 0s 241us/step - loss: 0.0951 - acc: 0.9583\n",
            "Epoch 71/100\n",
            "120/120 [==============================] - 0s 208us/step - loss: 0.1438 - acc: 0.9500\n",
            "Epoch 72/100\n",
            "120/120 [==============================] - 0s 226us/step - loss: 0.0993 - acc: 0.9583\n",
            "Epoch 73/100\n",
            "120/120 [==============================] - 0s 232us/step - loss: 0.0814 - acc: 0.9667\n",
            "Epoch 74/100\n",
            "120/120 [==============================] - 0s 255us/step - loss: 0.0920 - acc: 0.9500\n",
            "Epoch 75/100\n",
            "120/120 [==============================] - 0s 215us/step - loss: 0.0730 - acc: 0.9833\n",
            "Epoch 76/100\n",
            "120/120 [==============================] - 0s 253us/step - loss: 0.0677 - acc: 0.9667\n",
            "Epoch 77/100\n",
            "120/120 [==============================] - 0s 237us/step - loss: 0.0679 - acc: 0.9750\n",
            "Epoch 78/100\n",
            "120/120 [==============================] - 0s 219us/step - loss: 0.0686 - acc: 0.9833\n",
            "Epoch 79/100\n",
            "120/120 [==============================] - 0s 228us/step - loss: 0.0593 - acc: 0.9833\n",
            "Epoch 80/100\n",
            "120/120 [==============================] - 0s 247us/step - loss: 0.0900 - acc: 0.9667\n",
            "Epoch 81/100\n",
            "120/120 [==============================] - 0s 250us/step - loss: 0.1456 - acc: 0.9500\n",
            "Epoch 82/100\n",
            "120/120 [==============================] - 0s 221us/step - loss: 0.1901 - acc: 0.9250\n",
            "Epoch 83/100\n",
            "120/120 [==============================] - 0s 250us/step - loss: 0.1397 - acc: 0.9333\n",
            "Epoch 84/100\n",
            "120/120 [==============================] - 0s 235us/step - loss: 0.1001 - acc: 0.9750\n",
            "Epoch 85/100\n",
            "120/120 [==============================] - 0s 213us/step - loss: 0.0994 - acc: 0.9750\n",
            "Epoch 86/100\n",
            "120/120 [==============================] - 0s 212us/step - loss: 0.0720 - acc: 0.9750\n",
            "Epoch 87/100\n",
            "120/120 [==============================] - 0s 213us/step - loss: 0.0802 - acc: 0.9750\n",
            "Epoch 88/100\n",
            "120/120 [==============================] - 0s 242us/step - loss: 0.1318 - acc: 0.9667\n",
            "Epoch 89/100\n",
            "120/120 [==============================] - 0s 264us/step - loss: 0.1218 - acc: 0.9750\n",
            "Epoch 90/100\n",
            "120/120 [==============================] - 0s 205us/step - loss: 0.1630 - acc: 0.9500\n",
            "Epoch 91/100\n",
            "120/120 [==============================] - 0s 221us/step - loss: 0.1988 - acc: 0.9250\n",
            "Epoch 92/100\n",
            "120/120 [==============================] - 0s 225us/step - loss: 0.1071 - acc: 0.9583\n",
            "Epoch 93/100\n",
            "120/120 [==============================] - 0s 225us/step - loss: 0.0919 - acc: 0.9750\n",
            "Epoch 94/100\n",
            "120/120 [==============================] - 0s 224us/step - loss: 0.0903 - acc: 0.9667\n",
            "Epoch 95/100\n",
            "120/120 [==============================] - 0s 240us/step - loss: 0.1034 - acc: 0.9583\n",
            "Epoch 96/100\n",
            "120/120 [==============================] - 0s 231us/step - loss: 0.1362 - acc: 0.9500\n",
            "Epoch 97/100\n",
            "120/120 [==============================] - 0s 216us/step - loss: 0.0856 - acc: 0.9667\n",
            "Epoch 98/100\n",
            "120/120 [==============================] - 0s 234us/step - loss: 0.1403 - acc: 0.9417\n",
            "Epoch 99/100\n",
            "120/120 [==============================] - 0s 219us/step - loss: 0.1191 - acc: 0.9500\n",
            "Epoch 100/100\n",
            "120/120 [==============================] - 0s 242us/step - loss: 0.0573 - acc: 0.9917\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "oU6FK__61Qdn",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        },
        "outputId": "991fad39-b3ce-46eb-e76e-ac0af769528f"
      },
      "cell_type": "code",
      "source": [
        "#Accuracy of the predicted values\n",
        "from sklearn.metrics import classification_report,confusion_matrix\n",
        "print(classification_report(y_test_class,y_pred_class))\n",
        "print(confusion_matrix(y_test_class,y_pred_class))"
      ],
      "execution_count": 104,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00        11\n",
            "           1       0.93      1.00      0.96        13\n",
            "           2       1.00      0.83      0.91         6\n",
            "\n",
            "   micro avg       0.97      0.97      0.97        30\n",
            "   macro avg       0.98      0.94      0.96        30\n",
            "weighted avg       0.97      0.97      0.97        30\n",
            "\n",
            "[[11  0  0]\n",
            " [ 0 13  0]\n",
            " [ 0  1  5]]\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}