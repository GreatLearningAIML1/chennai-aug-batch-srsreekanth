{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4WH1Pr4KQlCh"
   },
   "source": [
    "### Build a DNN using Keras with `RELU` and `ADAM`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TbvI8LqlQlCl",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Load tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SPW-a-qYQlCp"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "74cQBsi5QlCw",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Collect Fashion mnist data from tf.keras.datasets "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8H4gOaeJQlCx"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
      "11493376/11490434 [==============================] - 3s 0us/step\n"
     ]
    }
   ],
   "source": [
    "(trainX, trainY),(testX, testY) = tf.keras.datasets.mnist.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "no7aWYZyQlC1",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Change train and test labels into one-hot vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UX6otc4wQlC2"
   },
   "outputs": [],
   "source": [
    "trainY = tf.keras.utils.to_categorical(trainY, num_classes=10)\n",
    "testY = tf.keras.utils.to_categorical(testY, num_classes=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QjNrRTdoQlC5",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Build the Graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CDJ9DHVNQlC7"
   },
   "source": [
    "#### Initialize model, reshape & normalize data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pCDQs_g1QlC8"
   },
   "outputs": [],
   "source": [
    "#Initialize Sequential model\n",
    "model = tf.keras.models.Sequential()\n",
    "\n",
    "#Reshape data from 2D to 1D -> 28x28 to 784\n",
    "model.add(tf.keras.layers.Reshape((784,),input_shape=(28,28,)))\n",
    "\n",
    "#Normalize the data\n",
    "model.add(tf.keras.layers.BatchNormalization())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kBGwTTilQlDD",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Add two fully connected layers with 200 and 100 neurons respectively with `relu` activations. Add a dropout layer with `p=0.25`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IXbfpfOzQlDF"
   },
   "outputs": [],
   "source": [
    "#Add 1st hidden layer\n",
    "model.add(tf.keras.layers.Dense(200, activation='relu'))\n",
    "\n",
    "#Add 2nd hidden layer\n",
    "model.add(tf.keras.layers.Dense(100, activation='relu'))\n",
    "\n",
    "#Add dropout layer\n",
    "\n",
    "#dp=model.add(tf.nn.dropout(0.25,keep_prob=1,noise_shape=None,seed=None,name=None))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5I8f5otcQlDJ"
   },
   "source": [
    "### Add the output layer with a fully connected layer with 10 neurons with `softmax` activation. Use `categorical_crossentropy` loss and `adam` optimizer and train the network. And, report the final validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "E25kuP_NQlDM"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "reshape (Reshape)            (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization (BatchNo (None, 784)               3136      \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 200)               157000    \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 200)               40200     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 100)               20100     \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 200)               20200     \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 100)               20100     \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 200)               20200     \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 100)               20100     \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 200)               20200     \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 100)               20100     \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 200)               20200     \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 100)               20100     \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 200)               20200     \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 100)               20100     \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 200)               20200     \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 100)               20100     \n",
      "_________________________________________________________________\n",
      "dense_15 (Dense)             (None, 200)               20200     \n",
      "_________________________________________________________________\n",
      "dense_16 (Dense)             (None, 100)               20100     \n",
      "_________________________________________________________________\n",
      "dense_17 (Dense)             (None, 200)               20200     \n",
      "_________________________________________________________________\n",
      "dense_18 (Dense)             (None, 100)               20100     \n",
      "_________________________________________________________________\n",
      "dense_19 (Dense)             (None, 200)               20200     \n",
      "_________________________________________________________________\n",
      "dense_20 (Dense)             (None, 100)               20100     \n",
      "_________________________________________________________________\n",
      "dense_21 (Dense)             (None, 200)               20200     \n",
      "_________________________________________________________________\n",
      "dense_22 (Dense)             (None, 100)               20100     \n",
      "_________________________________________________________________\n",
      "dense_23 (Dense)             (None, 200)               20200     \n",
      "_________________________________________________________________\n",
      "dense_24 (Dense)             (None, 100)               20100     \n",
      "_________________________________________________________________\n",
      "dense_25 (Dense)             (None, 200)               20200     \n",
      "_________________________________________________________________\n",
      "dense_26 (Dense)             (None, 100)               20100     \n",
      "_________________________________________________________________\n",
      "dense_27 (Dense)             (None, 200)               20200     \n",
      "_________________________________________________________________\n",
      "dense_28 (Dense)             (None, 100)               20100     \n",
      "_________________________________________________________________\n",
      "dense_29 (Dense)             (None, 10)                1010      \n",
      "_________________________________________________________________\n",
      "dense_30 (Dense)             (None, 10)                110       \n",
      "_________________________________________________________________\n",
      "dense_31 (Dense)             (None, 10)                110       \n",
      "_________________________________________________________________\n",
      "dense_32 (Dense)             (None, 10)                110       \n",
      "=================================================================\n",
      "Total params: 745,676\n",
      "Trainable params: 744,108\n",
      "Non-trainable params: 1,568\n",
      "_________________________________________________________________\n",
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/30\n",
      "60000/60000 [==============================] - 22s 371us/step - loss: 2.3021 - acc: 0.1094 - val_loss: 2.3010 - val_acc: 0.1135\n",
      "Epoch 2/30\n",
      "60000/60000 [==============================] - 21s 353us/step - loss: 2.3014 - acc: 0.1122 - val_loss: 2.3011 - val_acc: 0.1135\n",
      "Epoch 3/30\n",
      "60000/60000 [==============================] - 21s 353us/step - loss: 2.3014 - acc: 0.1124 - val_loss: 2.3013 - val_acc: 0.1135\n",
      "Epoch 4/30\n",
      "60000/60000 [==============================] - 21s 353us/step - loss: 2.3015 - acc: 0.1124 - val_loss: 2.3011 - val_acc: 0.1135\n",
      "Epoch 5/30\n",
      "60000/60000 [==============================] - 21s 355us/step - loss: 2.3015 - acc: 0.1123 - val_loss: 2.3015 - val_acc: 0.1135\n",
      "Epoch 6/30\n",
      "60000/60000 [==============================] - 21s 354us/step - loss: 2.3014 - acc: 0.1124 - val_loss: 2.3013 - val_acc: 0.1135\n",
      "Epoch 7/30\n",
      "60000/60000 [==============================] - 21s 355us/step - loss: 2.3015 - acc: 0.1124 - val_loss: 2.3010 - val_acc: 0.1135\n",
      "Epoch 8/30\n",
      "60000/60000 [==============================] - 21s 346us/step - loss: 2.3015 - acc: 0.1124 - val_loss: 2.3011 - val_acc: 0.1135\n",
      "Epoch 9/30\n",
      "60000/60000 [==============================] - 21s 344us/step - loss: 2.3015 - acc: 0.1124 - val_loss: 2.3014 - val_acc: 0.1135\n",
      "Epoch 10/30\n",
      "60000/60000 [==============================] - 21s 353us/step - loss: 2.3014 - acc: 0.1123 - val_loss: 2.3013 - val_acc: 0.1135\n",
      "Epoch 11/30\n",
      "60000/60000 [==============================] - 21s 347us/step - loss: 2.3015 - acc: 0.1124 - val_loss: 2.3009 - val_acc: 0.1135\n",
      "Epoch 12/30\n",
      "60000/60000 [==============================] - 21s 353us/step - loss: 2.3015 - acc: 0.1124 - val_loss: 2.3011 - val_acc: 0.1135\n",
      "Epoch 13/30\n",
      "60000/60000 [==============================] - 21s 346us/step - loss: 2.3015 - acc: 0.1124 - val_loss: 2.3010 - val_acc: 0.1135\n",
      "Epoch 14/30\n",
      "60000/60000 [==============================] - 20s 337us/step - loss: 2.3015 - acc: 0.1124 - val_loss: 2.3009 - val_acc: 0.1135\n",
      "Epoch 15/30\n",
      "60000/60000 [==============================] - 20s 337us/step - loss: 2.3014 - acc: 0.1124 - val_loss: 2.3013 - val_acc: 0.1135\n",
      "Epoch 16/30\n",
      "60000/60000 [==============================] - 20s 335us/step - loss: 2.3015 - acc: 0.1122 - val_loss: 2.3010 - val_acc: 0.1135\n",
      "Epoch 17/30\n",
      "60000/60000 [==============================] - 20s 335us/step - loss: 2.3014 - acc: 0.1124 - val_loss: 2.3011 - val_acc: 0.1135\n",
      "Epoch 18/30\n",
      "60000/60000 [==============================] - 20s 338us/step - loss: 2.3014 - acc: 0.1124 - val_loss: 2.3014 - val_acc: 0.1135\n",
      "Epoch 19/30\n",
      "60000/60000 [==============================] - 20s 341us/step - loss: 2.3015 - acc: 0.1122 - val_loss: 2.3010 - val_acc: 0.1135\n",
      "Epoch 20/30\n",
      "60000/60000 [==============================] - 20s 336us/step - loss: 2.3015 - acc: 0.1124 - val_loss: 2.3014 - val_acc: 0.1135\n",
      "Epoch 21/30\n",
      "60000/60000 [==============================] - 20s 337us/step - loss: 2.3015 - acc: 0.1124 - val_loss: 2.3009 - val_acc: 0.1135\n",
      "Epoch 22/30\n",
      "60000/60000 [==============================] - 20s 339us/step - loss: 2.3014 - acc: 0.1124 - val_loss: 2.3011 - val_acc: 0.1135\n",
      "Epoch 23/30\n",
      "60000/60000 [==============================] - 20s 336us/step - loss: 2.3014 - acc: 0.1124 - val_loss: 2.3012 - val_acc: 0.1135\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24/30\n",
      "60000/60000 [==============================] - 20s 331us/step - loss: 2.3014 - acc: 0.1124 - val_loss: 2.3011 - val_acc: 0.1135\n",
      "Epoch 25/30\n",
      "60000/60000 [==============================] - 20s 330us/step - loss: 2.3015 - acc: 0.1124 - val_loss: 2.3013 - val_acc: 0.1135\n",
      "Epoch 26/30\n",
      "60000/60000 [==============================] - 20s 336us/step - loss: 2.3014 - acc: 0.1122 - val_loss: 2.3013 - val_acc: 0.1135\n",
      "Epoch 27/30\n",
      "60000/60000 [==============================] - 20s 333us/step - loss: 2.3015 - acc: 0.1124 - val_loss: 2.3009 - val_acc: 0.1135\n",
      "Epoch 28/30\n",
      "60000/60000 [==============================] - 20s 331us/step - loss: 2.3015 - acc: 0.1124 - val_loss: 2.3010 - val_acc: 0.1135\n",
      "Epoch 29/30\n",
      "60000/60000 [==============================] - 20s 332us/step - loss: 2.3014 - acc: 0.1121 - val_loss: 2.3012 - val_acc: 0.1135\n",
      "Epoch 30/30\n",
      "60000/60000 [==============================] - 20s 336us/step - loss: 2.3014 - acc: 0.1124 - val_loss: 2.3014 - val_acc: 0.1135\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x2acfb5bb978>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Add OUTPUT layer\n",
    "model.add(tf.keras.layers.Dense(10, activation='softmax'))\n",
    "\n",
    "#Create optimizer with non-default learning rate\n",
    "sgd_optimizer = tf.keras.optimizers.SGD(lr=0.03)\n",
    "\n",
    "#Compile the model\n",
    "model.compile(optimizer=sgd_optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "model.summary()\n",
    "model.fit(trainX,trainY,          \n",
    "          validation_data=(testX,testY),\n",
    "          epochs=30,\n",
    "          batch_size=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vJl2GM7QQvs7"
   },
   "source": [
    "## Word Embeddings in Python with Gensim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9QoSmK_yQydL"
   },
   "source": [
    "In this, you will practice how to train and load word embedding models for natural language processing applications in Python using Gensim.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lqHUk1kBQ2Pl"
   },
   "source": [
    "1. How to train your own word2vec word embedding model on text data.\n",
    "2. How to visualize a trained word embedding model using Principal Component Analysis.\n",
    "3. How to load pre-trained word2vec word embedding models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "x3EEkH5mQ6F0"
   },
   "source": [
    "### Run the below two commands to install gensim and the wiki dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Z_dPUh8YQ6Yi"
   },
   "outputs": [],
   "source": [
    "#!pip install --upgrade gensim --user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-9-xPiuTQ-nD"
   },
   "outputs": [],
   "source": [
    "#!pip install wikipedia --user"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Te2mI1FHQ_I1"
   },
   "source": [
    "### Import gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qQQwRUJ0RAlJ"
   },
   "outputs": [],
   "source": [
    "import wikipedia \n",
    "import gensim "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5dXgBNlORBGP"
   },
   "source": [
    "### Obtain Text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xdnvuDLCRDO4"
   },
   "source": [
    "Import search and page functions from wikipedia module\n",
    "search(/key word/): search function takes keyword as argument and gives top 10 article titles matching the given keyword.\n",
    "\n",
    "page(/title of article/): page function takes page title as argument and gives content in the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VgB7SX-TRHNT"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"Here We Go Round the Mulberry Bush\" (also titled \"Mulberry Bush\" or \"This Is the Way\") is an English nursery rhyme and singing game. It has a Roud Folk Song Index number of 7882. The same tune is also used for \"Lazy Mary, Will You Get Up\" and \"Nuts in May\". A variant is used for \"The Wheels on the Bus\".\n",
      "\n",
      "\n",
      "== Lyrics ==\n",
      "The most common modern version of the rhyme is:\n",
      "\n",
      "\n",
      "== Score ==\n",
      "\n",
      "\n",
      "== Origins and meaning ==\n",
      "The rhyme was first recorded by James Orchard Halliwell as an English children's game in the mid-19th century. He noted that there was a similar game with the lyrics \"Here we go round the bramble bush\". The bramble bush may be an earlier version, possibly changed because of the difficulty of the alliteration, since mulberries do not grow on bushes.Halliwell said subsequent verses included: \"This is the way we wash our clothes\", \"This is the way we dry our clothes\", \"This is the way we mend our shoes\", \"This is the way the gentlemen walk\" and \"This is the way the ladies walk.The song and associated game is traditional, and has parallels in Scandinavia and in the Netherlands (the bush is a juniper in Scandinavia).Local historian R. S. Duncan suggests that the song originated with female prisoners at HMP Wakefield. A sprig was taken from Hatfeild Hall (Normanton Golf Club) in Stanley, Wakefield, and grew into a fully mature mulberry tree around which prisoners exercised in the moonlight.The Christmas carol, \"As I Sat on a Sunny Bank\", collected by Cecil Sharp in Worcestershire, has a very similar melody; as does the related \"I Saw Three Ships.\"Another possible interpretation of the rhyme is that it references Britain's struggles to produce silk, mulberry trees being a key habitat for the cultivation of silkworms. As Bill Bryson explains, Britain in the eighteenth and nineteenth centuries tried to emulate the success of the Chinese in silk production but the industry was held back by periodic harsh winters and mulberry trees proved too sensitive to frost to thrive.   The  traditional lyrics \"Here we go round the mulberry bush / On a cold and frosty morning\" may therefore be a joke about the problems faced by the industry.\n",
      "\n",
      "\n",
      "== Game and song ==\n",
      "The simple game involves holding hands in a circle and moving around to the first verse, which is alternated with the specific verse, where the players break up to imitate various appropriate actions.A variant of this rhyme is \"Nuts in May\", sharing the tune as well as the traditional closing line \"On a cold and frosty morning\".\n",
      "\n",
      "\n",
      "== Recordings ==\n",
      "In 1938, a song called \"Stop Beatin' Round the Mulberry Bush\", with lyrics by Bickley Reichner and music by Clay Boland and built around the basic melody of the nursery rhyme, was popular with recordings by bands such as Count Basie, Jack Hylton, Nat Gonella, and Joe Loss. That version became popular again in 1953, when it was recorded by Bill Haley & His Comets. \n",
      "A version was recorded in Western Armenian in the 2017 album, My First Armenian Songbook, arranged by Karenn Presti and sung by Stephanie Betjemann.\n",
      "\n",
      "\n",
      "=== The Merry-Go-Round ===\n",
      "The Merry-Go-Round is a song with the same tune as \"Here We Go Round the Mulberry Bush\", but some notes are removed. The song tells the story of several children on a merry-go-round that—in a sadistic twist—collapses because so many children are riding it. The circle game that accompanies it is similar to the one for Ring Around the Rosie, as described below.\n",
      "\n",
      "The verse is usually repeated a second time.\n",
      "The circle singing game that accompanies these verses also changes by region, but the most common form consists of participants standing in a circle and holding hands, followed by skipping in one direction as they sing the tune that accompanies these verses. As the word collapsed in the second verse is sung, the group usually falls down into a heap.\n",
      "\n",
      "\n",
      "== See also ==\n",
      "\n",
      "List of nursery rhymes\n",
      "Nuts in May (rhyme)\n",
      "The Wheels on the Bus\n",
      "Pop Goes the Weasel\n",
      "\n",
      "\n",
      "== Notes ==\n",
      "\n",
      "\n",
      "== External links ==\n",
      "A full text\n"
     ]
    }
   ],
   "source": [
    "## Usage: \n",
    "\n",
    "from wikipedia import search, page\n",
    "titles = search(\"<Key word goes here>\")\n",
    "wikipage = page(titles[0])\n",
    "print (wikipage.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "q54h4vNaRJXe"
   },
   "source": [
    "### Print the top 10 titles for the keyword `Machine Learning`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yuyLmsnsRK6f"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "a3faXGqLRQx1"
   },
   "source": [
    "### Get the content from the first title from the above obtained 10 titles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HOqiYzrqRQ-M"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "C8ugHUzSRTFH"
   },
   "source": [
    "### Create a list with name `documents` and append all the words in the 10 pages' content using the above 10 titles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nBKXHtfCRTZw"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "eyo1zvzERVgG"
   },
   "source": [
    "### Build the gensim model for word2vec with by considering all the words with frequency >=1 with embedding size=50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hMkxm5eHRVts"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'documents' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-35-a6f021419233>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgensim\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mWord2Vec\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdocuments\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m50\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mwindow\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mmin_count\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mworkers\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdocuments\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtotal_examples\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdocuments\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'documents' is not defined"
     ]
    }
   ],
   "source": [
    "model = gensim.models.Word2Vec(documents,size=50,window=10,min_count=2,workers=10)\n",
    "\n",
    "model.train(documents, total_examples=len(documents), epochs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FyLV4F6uRYZx"
   },
   "source": [
    "### Exploring the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6iuSUoJERYhc"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XxUuShBTRaYH"
   },
   "source": [
    "#### Check how many words in the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "S4lMhzWlRamw"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KshrfDklReIl"
   },
   "source": [
    "### Get an embedding for word `SVM`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YiwT1OjARg6e"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7COLA1LzRhTj"
   },
   "source": [
    "### Finding most similar words for word `learning`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tEsMKWY6Ri3n"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "K8QjNuDKRlvt"
   },
   "source": [
    "### Find the word which is not like others from `machine, svm, ball, learning`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "oBudwDHtRl3u"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cjNQ0yMNRn6D"
   },
   "source": [
    "### Save the model with name `word2vec-wiki-10`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bynMebY5RoLn"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cDeem0T5RqTn"
   },
   "source": [
    "### Load the model `word2vec-wiki-10`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VJJk04KfRqen"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "R7_ExternalLab_Questions.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
