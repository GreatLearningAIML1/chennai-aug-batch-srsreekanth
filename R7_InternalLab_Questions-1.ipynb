{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MyfMmMnPJjvn"
   },
   "source": [
    "## Train a simple convnet on the Fashion MNIST dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zjcGOJhcJjvp"
   },
   "source": [
    "In this, we will see how to deal with image data and train a convnet for image classification task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jR0Pl2XjJjvq"
   },
   "source": [
    "### Load the  `fashion_mnist`  dataset\n",
    "\n",
    "** Use keras.datasets to load the dataset **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Qr75v_UYJjvs"
   },
   "outputs": [],
   "source": [
    "from keras.datasets import fashion_mnist\n",
    "(x_train, y_train), (x_test, y_test) = fashion_mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hTI42-0qJjvw"
   },
   "source": [
    "### Find no.of samples are there in training and test datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "g2sf67VoJjvx"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data shape :  (60000, 28, 28) (60000,)\n",
      "Testing data shape :  (10000, 28, 28) (10000,)\n",
      "Total number of outputs :  10\n",
      "Output classes :  [0 1 2 3 4 5 6 7 8 9]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from keras.utils import to_categorical\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "print('Training data shape : ', x_train.shape, y_train.shape)\n",
    "\n",
    "print('Testing data shape : ', x_test.shape, y_test.shape)\n",
    "\n",
    "# Find the unique numbers from the train labels\n",
    "classes = np.unique(y_train)\n",
    "nClasses = len(classes)\n",
    "print('Total number of outputs : ', nClasses)\n",
    "print('Output classes : ', classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zewyDcBlJjv1"
   },
   "outputs": [],
   "source": [
    "##60000 represents the number of images in the train dataset and (28, 28) represents the size of the image: 28 x 28 pixel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WytT2eRnJjv4"
   },
   "source": [
    "### Find dimensions of an image in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XycQGBSGJjv5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x1f142fb3b38>"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "image_index = 7777 # You may select anything up to 60,000\n",
    "\n",
    "print(y_train[image_index]) # The label is 8\n",
    "\n",
    "plt.imshow(x_train[image_index], cmap='Greys')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5jtdZ7RqJjv8"
   },
   "source": [
    "### Convert train and test labels to one hot vectors\n",
    "\n",
    "** check `keras.utils.to_categorical()` **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original label: 9\n",
      "After conversion to one-hot: [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n"
     ]
    }
   ],
   "source": [
    "# Change the labels from categorical to one-hot encoding\n",
    "train_Y_one_hot = to_categorical(y_train)\n",
    "test_Y_one_hot = to_categorical(y_test)\n",
    "\n",
    "# Display the change for category label using one-hot encoding\n",
    "print('Original label:', y_train[0])\n",
    "print('After conversion to one-hot:', train_Y_one_hot[0])\n",
    "\n",
    "y_train=train_Y_one_hot\n",
    "y_test=test_Y_one_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sAD3q5I6Jjv9"
   },
   "outputs": [],
   "source": [
    "#from keras.utils.np_utils import to_categorical\n",
    "#y_train= to_categorical(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mgHSCXy3JjwA"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xO5BRBzBJjwD"
   },
   "source": [
    "### Normalize both the train and test image data from 0-255 to 0-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3fUQpMHxJjwE"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape: (60000, 28, 28, 1)\n",
      "Number of images in x_train 60000\n",
      "Number of images in x_test 10000\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Reshaping the array to 4-dims so that it can work with the Keras API\n",
    "\n",
    "x_train = x_train.reshape(-1, 28,28, 1)\n",
    "x_test = x_test.reshape(-1, 28,28, 1)\n",
    "x_train.shape, x_test.shape\n",
    "\n",
    "input_shape = (28, 28, 1)\n",
    "\n",
    "# Making sure that the values are float so that we can get decimal points after division\n",
    "\n",
    "x_train = x_train.astype('float32')\n",
    "\n",
    "x_test = x_test.astype('float32')\n",
    "\n",
    "# Normalizing the RGB codes by dividing it to the max RGB value.\n",
    "\n",
    "x_train /= 255\n",
    "\n",
    "x_test /= 255\n",
    "\n",
    "print('x_train shape:', x_train.shape)\n",
    "\n",
    "print('Number of images in x_train', x_train.shape[0])\n",
    "\n",
    "print('Number of images in x_test', x_test.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Okwo_SB5JjwI"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "da5-DwgrJjwM"
   },
   "source": [
    "### Reshape the data from 28x28 to 28x28x1 to match input dimensions in Conv2D layer in keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LPGVQ-JJJjwN"
   },
   "outputs": [],
   "source": [
    "#done above"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OFRRTJq8JjwQ"
   },
   "source": [
    "### Import the necessary layers from keras to build the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dWTZYnKSJjwR"
   },
   "outputs": [],
   "source": [
    "# Importing the required Keras modules containing model and layers\n",
    "\n",
    "import keras\n",
    "from keras.models import Sequential,Input,Model\n",
    "from keras.layers import Dense, Dropout, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.layers.advanced_activations import LeakyReLU\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "C18AoS7eJjwU"
   },
   "source": [
    "### Build a model \n",
    "\n",
    "** with 2 Conv layers having `32 3*3 filters` in both convolutions with `relu activations` and `flatten` before passing the feature map into 2 fully connected layers (or Dense Layers) having 128 and 10 neurons with `relu` and `softmax` activations respectively. Now, using `categorical_crossentropy` loss with `adam` optimizer train the model with early stopping `patience=5` and no.of `epochs=10`. **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "epochs = 20\n",
    "num_classes = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "fashion_model = Sequential()\n",
    "fashion_model.add(Conv2D(32, kernel_size=(3, 3),activation='linear',input_shape=(28,28,1),padding='same'))\n",
    "fashion_model.add(LeakyReLU(alpha=0.1))\n",
    "fashion_model.add(MaxPooling2D((2, 2),padding='same'))\n",
    "fashion_model.add(Conv2D(64, (3, 3), activation='linear',padding='same'))\n",
    "fashion_model.add(LeakyReLU(alpha=0.1))\n",
    "fashion_model.add(MaxPooling2D(pool_size=(2, 2),padding='same'))\n",
    "fashion_model.add(Conv2D(128, (3, 3), activation='linear',padding='same'))\n",
    "fashion_model.add(LeakyReLU(alpha=0.1))                  \n",
    "fashion_model.add(MaxPooling2D(pool_size=(2, 2),padding='same'))\n",
    "fashion_model.add(Flatten())\n",
    "fashion_model.add(Dense(128, activation='linear'))\n",
    "fashion_model.add(LeakyReLU(alpha=0.1))                  \n",
    "fashion_model.add(Dense(num_classes, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_12 (Conv2D)           (None, 32, 28, 1)         8096      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_1 (LeakyReLU)    (None, 32, 28, 1)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_6 (MaxPooling2 (None, 32, 14, 1)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_13 (Conv2D)           (None, 64, 14, 1)         18496     \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_2 (LeakyReLU)    (None, 64, 14, 1)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_7 (MaxPooling2 (None, 64, 7, 1)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_14 (Conv2D)           (None, 128, 7, 1)         73856     \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_3 (LeakyReLU)    (None, 128, 7, 1)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_8 (MaxPooling2 (None, 128, 4, 1)         0         \n",
      "_________________________________________________________________\n",
      "flatten_5 (Flatten)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 128)               65664     \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_4 (LeakyReLU)    (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 10)                1290      \n",
      "=================================================================\n",
      "Total params: 167,402\n",
      "Trainable params: 167,402\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#compile model\n",
    "fashion_model.compile(loss=keras.losses.categorical_crossentropy, optimizer=keras.optimizers.Adam(),metrics=['accuracy'])\n",
    "fashion_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 48000 samples, validate on 12000 samples\n",
      "Epoch 1/20\n",
      "48000/48000 [==============================] - 33s 682us/step - loss: 0.5902 - acc: 0.7808 - val_loss: 0.4345 - val_acc: 0.8470\n",
      "Epoch 2/20\n",
      "48000/48000 [==============================] - 32s 661us/step - loss: 0.3726 - acc: 0.8655 - val_loss: 0.3751 - val_acc: 0.8674\n",
      "Epoch 3/20\n",
      "48000/48000 [==============================] - 33s 685us/step - loss: 0.3281 - acc: 0.8801 - val_loss: 0.3251 - val_acc: 0.8822\n",
      "Epoch 4/20\n",
      "48000/48000 [==============================] - 33s 684us/step - loss: 0.2996 - acc: 0.8880 - val_loss: 0.3098 - val_acc: 0.8877\n",
      "Epoch 5/20\n",
      "48000/48000 [==============================] - 33s 687us/step - loss: 0.2786 - acc: 0.8962 - val_loss: 0.3062 - val_acc: 0.8882\n",
      "Epoch 6/20\n",
      "48000/48000 [==============================] - 33s 689us/step - loss: 0.2631 - acc: 0.9009 - val_loss: 0.3130 - val_acc: 0.8859\n",
      "Epoch 7/20\n",
      "48000/48000 [==============================] - 33s 689us/step - loss: 0.2499 - acc: 0.9064 - val_loss: 0.2867 - val_acc: 0.8949\n",
      "Epoch 8/20\n",
      "48000/48000 [==============================] - 35s 739us/step - loss: 0.2344 - acc: 0.9121 - val_loss: 0.2915 - val_acc: 0.8965\n",
      "Epoch 9/20\n",
      "48000/48000 [==============================] - 34s 715us/step - loss: 0.2206 - acc: 0.9167 - val_loss: 0.2797 - val_acc: 0.8998\n",
      "Epoch 10/20\n",
      "48000/48000 [==============================] - 34s 716us/step - loss: 0.2065 - acc: 0.9211 - val_loss: 0.2844 - val_acc: 0.8981\n",
      "Epoch 11/20\n",
      "48000/48000 [==============================] - 34s 717us/step - loss: 0.1972 - acc: 0.9260 - val_loss: 0.2849 - val_acc: 0.8998\n",
      "Epoch 12/20\n",
      "48000/48000 [==============================] - 35s 731us/step - loss: 0.1831 - acc: 0.9306 - val_loss: 0.2995 - val_acc: 0.8981\n",
      "Epoch 13/20\n",
      "48000/48000 [==============================] - 35s 721us/step - loss: 0.1704 - acc: 0.9345 - val_loss: 0.2847 - val_acc: 0.9022\n",
      "Epoch 14/20\n",
      "48000/48000 [==============================] - 31s 654us/step - loss: 0.1634 - acc: 0.9375 - val_loss: 0.3206 - val_acc: 0.9001\n",
      "Epoch 15/20\n",
      "48000/48000 [==============================] - 30s 634us/step - loss: 0.1516 - acc: 0.9412 - val_loss: 0.3129 - val_acc: 0.9047\n",
      "Epoch 16/20\n",
      "48000/48000 [==============================] - 33s 683us/step - loss: 0.1435 - acc: 0.9454 - val_loss: 0.3100 - val_acc: 0.9021\n",
      "Epoch 17/20\n",
      "48000/48000 [==============================] - 36s 745us/step - loss: 0.1300 - acc: 0.9507 - val_loss: 0.3266 - val_acc: 0.9012\n",
      "Epoch 18/20\n",
      "48000/48000 [==============================] - 37s 762us/step - loss: 0.1240 - acc: 0.9521 - val_loss: 0.3454 - val_acc: 0.9015\n",
      "Epoch 19/20\n",
      "48000/48000 [==============================] - 35s 726us/step - loss: 0.1158 - acc: 0.9550 - val_loss: 0.3592 - val_acc: 0.8963\n",
      "Epoch 20/20\n",
      "48000/48000 [==============================] - 33s 681us/step - loss: 0.1095 - acc: 0.9573 - val_loss: 0.3632 - val_acc: 0.8998\n"
     ]
    }
   ],
   "source": [
    "#fit the model\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "train_X,valid_X,train_label,valid_label = train_test_split(x_train, train_Y_one_hot, test_size=0.2, random_state=13)\n",
    "\n",
    "fashion_train = fashion_model.fit(train_X, train_label, batch_size=batch_size,epochs=epochs,verbose=1,validation_data=(valid_X, valid_label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DORCLgSwJjwV"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ju69vKdIJjwX"
   },
   "source": [
    "### Now, to the above model add `max` pooling layer of `filter size 2x2` and `dropout` layer with `p=0.25` after the 2 conv layers and run the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "L2hAP94vJjwY"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 48000 samples, validate on 12000 samples\n",
      "Epoch 1/20\n",
      "48000/48000 [==============================] - 32s 673us/step - loss: 0.1040 - acc: 0.9602 - val_loss: 0.3903 - val_acc: 0.8982\n",
      "Epoch 2/20\n",
      "48000/48000 [==============================] - 33s 688us/step - loss: 0.0962 - acc: 0.9630 - val_loss: 0.3927 - val_acc: 0.9021\n",
      "Epoch 3/20\n",
      "48000/48000 [==============================] - 34s 712us/step - loss: 0.0908 - acc: 0.9646 - val_loss: 0.3946 - val_acc: 0.9008\n",
      "Epoch 4/20\n",
      "48000/48000 [==============================] - 34s 704us/step - loss: 0.0875 - acc: 0.9653 - val_loss: 0.4231 - val_acc: 0.8993\n",
      "Epoch 5/20\n",
      "48000/48000 [==============================] - 32s 668us/step - loss: 0.0789 - acc: 0.9696 - val_loss: 0.4449 - val_acc: 0.8987\n",
      "Epoch 6/20\n",
      "48000/48000 [==============================] - 34s 715us/step - loss: 0.0769 - acc: 0.9698 - val_loss: 0.4551 - val_acc: 0.8976\n",
      "Epoch 7/20\n",
      "48000/48000 [==============================] - 33s 686us/step - loss: 0.0704 - acc: 0.9726 - val_loss: 0.4548 - val_acc: 0.9002\n",
      "Epoch 8/20\n",
      "48000/48000 [==============================] - 35s 726us/step - loss: 0.0684 - acc: 0.9733 - val_loss: 0.4713 - val_acc: 0.9014\n",
      "Epoch 9/20\n",
      "48000/48000 [==============================] - 33s 689us/step - loss: 0.0639 - acc: 0.9748 - val_loss: 0.4915 - val_acc: 0.9015\n",
      "Epoch 10/20\n",
      "48000/48000 [==============================] - 37s 763us/step - loss: 0.0612 - acc: 0.9759 - val_loss: 0.4956 - val_acc: 0.8999\n",
      "Epoch 11/20\n",
      "48000/48000 [==============================] - 33s 689us/step - loss: 0.0586 - acc: 0.9770 - val_loss: 0.5142 - val_acc: 0.8964\n",
      "Epoch 12/20\n",
      "48000/48000 [==============================] - 37s 762us/step - loss: 0.0601 - acc: 0.9766 - val_loss: 0.5287 - val_acc: 0.8941- E\n",
      "Epoch 13/20\n",
      "48000/48000 [==============================] - 33s 686us/step - loss: 0.0554 - acc: 0.9783 - val_loss: 0.5180 - val_acc: 0.9027\n",
      "Epoch 14/20\n",
      "48000/48000 [==============================] - 30s 625us/step - loss: 0.0522 - acc: 0.9803 - val_loss: 0.5371 - val_acc: 0.9010\n",
      "Epoch 15/20\n",
      "48000/48000 [==============================] - 32s 673us/step - loss: 0.0516 - acc: 0.9799 - val_loss: 0.5457 - val_acc: 0.8976\n",
      "Epoch 16/20\n",
      "48000/48000 [==============================] - 35s 728us/step - loss: 0.0473 - acc: 0.9823 - val_loss: 0.5685 - val_acc: 0.8978\n",
      "Epoch 17/20\n",
      "48000/48000 [==============================] - 34s 703us/step - loss: 0.0509 - acc: 0.9816 - val_loss: 0.5652 - val_acc: 0.8964\n",
      "Epoch 18/20\n",
      "48000/48000 [==============================] - 35s 739us/step - loss: 0.0443 - acc: 0.9836 - val_loss: 0.5944 - val_acc: 0.8996\n",
      "Epoch 19/20\n",
      "48000/48000 [==============================] - 36s 751us/step - loss: 0.0472 - acc: 0.9823 - val_loss: 0.6192 - val_acc: 0.8968\n",
      "Epoch 20/20\n",
      "48000/48000 [==============================] - 38s 790us/step - loss: 0.0451 - acc: 0.9834 - val_loss: 0.6003 - val_acc: 0.9002\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "fashion_train = fashion_model.fit(train_X, train_label, batch_size=batch_size,epochs=epochs,verbose=1,validation_data=(valid_X, valid_label))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lGTA3bfEJjwa"
   },
   "source": [
    "### Now, to the above model, lets add Data Augmentation "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "F6gX8n5SJjwb"
   },
   "source": [
    "### Import the ImageDataGenrator from keras and fit the training images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Cbz4uHBuJjwc"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\RB00001\\Anaconda2\\2.2\\Anaconda2\\lib\\site-packages\\keras_preprocessing\\image\\image_data_generator.py:924: UserWarning: Expected input to be images (as Numpy array) following the data format convention \"channels_first\" (channels on axis 1), i.e. expected either 1, 3 or 4 channels on axis 1. However, it was passed an array with shape (60000, 28, 28, 1) (28 channels).\n",
      "  ' channels).')\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "from keras import backend as K\n",
    "K.set_image_dim_ordering('th')\n",
    "# load data\n",
    "datagen = ImageDataGenerator()\n",
    "# fit parameters from data\n",
    "datagen.fit(x_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pl-8dOo7Jjwf"
   },
   "source": [
    "#### Showing 5 versions of the first image in training dataset using image datagenerator.flow()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DpI1_McYJjwg",
    "outputId": "6722631e-c925-448c-c780-93a3100249bc",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\RB00001\\Anaconda2\\2.2\\Anaconda2\\lib\\site-packages\\keras_preprocessing\\image\\numpy_array_iterator.py:127: UserWarning: NumpyArrayIterator is set to use the data format convention \"channels_first\" (channels on axis 1), i.e. expected either 1, 3, or 4 channels on axis 1. However, it was passed an array with shape (1, 28, 28, 1) (28 channels).\n",
      "  str(self.x.shape[channels_axis]) + ' channels).')\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXIAAABcCAYAAABz9T77AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvOIA7rQAACNRJREFUeJzt3buPVOUfx/H3yv0ioAiRiwENEmyIBBWQSwJBoh2BBFtCaOj5DywgUttZWktBBIw9GCGE7BY0itwJFwGjICCwFuOHMzPngPBjf7P7LO9Xs+4yc2bmE/N8n3lup29wcBBJUrleGe43IEl6MTbkklQ4G3JJKpwNuSQVzoZckgpnQy5JhbMhl6TC2ZBLUuFsyCWpcGN7+WJ9fX0vxTbSwcHBvmd9rJnUmUkzc6kzkxZ75JJUOBtySSqcDbkkFc6GXJIKZ0MuSYXr6aqVXujra03udp+z/uqrrwKwZs0aAA4dOtT4vDFjxgDw4MGDZ36tGKlnu5tJnZk0M5e6EjKxRy5JhRt1PfJXXmnVpocPHwKwaNEiAHbu3AnAX3/9BcDt27cBuHv3LgA//fQT0Fw1UyVz7fze/dhU3pHGTOrMpJm51JWQiT1ySSrcqOuRp4Klem7YsAGAjRs3AnDhwgUAJkyYAMDkyZMB+OSTTwD4+uuvAbhy5crja2acKteMqVOnAvDo0SMA7ty5M5QfZciYSZ2ZNDOXuhIysUcuSYUbdT3y+/fvd/z+4YcfArBw4UKgqq4Zm/r+++8BWLZsGQBffvklAMePH398jYGBAQBOnToFwEcffdRx7SNHjgBw9OjRIfwkQ8dM6sykmbnUlZCJPXJJKtyo6ZF3r/XM+NQHH3wAwB9//AHAlClTAFi8eHHHz2PHjgHw888/A9VYFcCqVasA2LJlCwB///13x3Mye33v3r2h/VAvyEzqzKSZudSVlIk9ckkqnA25JBWur5fbYofyEPjurayRz/Pjjz8C1YRE9/Oy8L57IiOL+bP8B+DEiRNA9RUpz/30008BeOeddwCYN29e3sOwHIxvJo3Xavx7aZn8+57Mpf45zAR75JJUvGInO//rm8TNmzcBmDNnDlBto82i/bFjWx89ExCpmpMmTQI6q+fatWsB+Pjjj4FqmdHs2bMBOHz48It8lCFjJnVm0sxc6krOxB65JBWu2B75f8k22VS6/MyW199//x2A3377DajGvVKV28fL8txcM9tqU2Hfeuut/8+HGGJmUmcmzcylbiRnYo9ckgpXbI+8+xjIVLSMT82dOxeoFtTnZ8azMrOcajpjxgygqqaplADjx48Hqg0A06dPB6C/v7/jNbNRYLiYSZ2ZNDOXupIzsUcuSYUrtkeecafuIyY///xzAN58800Arl27BtRnjrOtNmNRqaaprtkyC9VsdK4xc+ZMAL766isA3n///Y7HDRczqTOTZuZSV3Im9sglqXDF7uxMpeq+NdKKFSsA+O6774BqrWd3lc2NU7PWM+NY48aN6/gJVaXNOtLIc/ft2wfAN998AwzfzjQzqRstmYC5NDGTFnvkklS4YR2Uyixx98Hs+XvGlNp3REXTDU0BDh48CFQ3Qk31zCxxvoFknCuvPXHixI7XbNf9PvKcpUuXAtX60aFgJnVm0sxc6l7WTOyRS1LhhqVH3j229KRK+DTr1q0DYOvWrQCsXr0aqNZwZnwqVTPjX3nNPC7vJTPLqaLtcwfdN0DNNf/880+gOhz+wIEDz/05wkzqzKSZudS97JnYI5ekwo2oVSuvv/46UO2gevfddzt+h6pS5XZK2V2VsbCMPWV95qVLl4BqxjiVL+s2s9Yzu65y09P22zKlUmc8K+NXueaVK1cAeO+994ChnXU3k7qXMRMwlyZm0mKPXJIKNyw98pUrVwLwxRdfADBr1iygOpsgY04Za7p169bja2TsK9Uu1S+z0plRPnXqFADbtm0D4Pjx40C11vO1114D6nf7OH36dMfjoDoPIeNaqcypsNOmTet4T/9Lj8JMKmbSzFzqzKTFHrkkFa6nPfKxY8cOAhw9ehSo7rSRatk9+xupolBVx245PeyNN94AYPv27QBs2rQJgF27dgHV+FZ2UP36669AVTUzhpbxLqgqdMavUlnze8a5FixYADxfj8JM6sykmbnUmUmLPXJJKlxPe+Q7duwYBNi7dy8Av/zyC1CNC+Vn1l9G+xkFqZLnz58HqmqYMbHMNOekss2bNwPVWs6MX+W1li9f3vEzz2+/E3b+ltnpyBha3l/G6c6dO/fMPQozqTOTZuZSZyb/Xu9p/yhJGvl6urPz6tWrQFX5Mi6UdZv5eypbqlVmcAFu3LgBwNmzZzsem3GujFNlJnr//v0ADAwMAFX1zPrSVMnMYmfNaPvOsIxXdY9fpXrmfWYd6vMwkzozaWYudWbSYo9ckgrX0x75xYsXgerMgQsXLgDV2byZHU4lu379OlCdKgbV+QYZ80pFy3hVKnLGoHKN7JDKCWap1DkPONfL49tPLEsl7d7hlTGz7MrKXT2eh5nUmUkzc6kzkxZ75JJUOBtySSpcT4dWTp48CcC3334LwI4dO4BquU8W0GdyIZMO7UuF8hUkkwFZ2J/JjWwAyFetbAS4fPlyx9/zuHyt6n7N9qVC+Vr2pMmLt99+G6gOunkeZlJnJs3Mpc5MWuyRS1LhhvUY288++wyA3bt3AzB79mygmhxItUqlg6padh/unr9n+U4+V/eNT/O8/J7Ht71HoLkS5rlZKpSJif7+fqA6TOdFjuE0EzN5EnOpM5MWe+SSVLie9sjHjBkzCM03PgVYv349AHv27AGqapottFAtAUq17L7dUmSjQD5flinltXNLpfbDc9of375UKGNiee0ffvgBqI61zMHxbdd45h6FmdSZSTNzqTOTfz/D0/5RkjTyjahbvT3JkiVLHv939wL/+fPnA3DmzBmgqno5PGc4DOWtqp7ETOpGcyZgLk3MpMUeuSQVrogeeWl60aMojZnU9apHXhr/X6mzRy5Jo5wNuSQVzoZckgrX0zFySdLQs0cuSYWzIZekwtmQS1LhbMglqXA25JJUOBtySSqcDbkkFc6GXJIKZ0MuSYWzIZekwtmQS1LhbMglqXA25JJUOBtySSqcDbkkFc6GXJIKZ0MuSYWzIZekwtmQS1LhbMglqXA25JJUOBtySSqcDbkkFe4f/52oiTAUWyMAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 5 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "gen = datagen.flow(x_train[0:1], batch_size=1)\n",
    "for i in range(1, 6):\n",
    "    plt.subplot(1,5,i)\n",
    "    plt.axis(\"off\")\n",
    "    plt.imshow(gen.next().squeeze(), cmap='gray')\n",
    "    plt.plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dmPl5yE8Jjwm"
   },
   "source": [
    "### Run the above model using fit_generator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "44ZnDdJYJjwn"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "    2/60000 [..............................] - ETA: 1:17:40 - loss: 0.2473 - acc: 0.9688"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\RB00001\\Anaconda2\\2.2\\Anaconda2\\lib\\site-packages\\ipykernel_launcher.py:5: UserWarning: The semantics of the Keras 2 argument `steps_per_epoch` is not the same as the Keras 1 argument `samples_per_epoch`. `steps_per_epoch` is the number of batches to draw from the generator at each epoch. Basically steps_per_epoch = samples_per_epoch/batch_size. Similarly `nb_val_samples`->`validation_steps` and `val_samples`->`steps` arguments have changed. Update your method calls accordingly.\n",
      "  \"\"\"\n",
      "C:\\Users\\RB00001\\Anaconda2\\2.2\\Anaconda2\\lib\\site-packages\\ipykernel_launcher.py:5: UserWarning: Update your `fit_generator` call to the Keras 2 API: `fit_generator(<keras_pre..., 60000, validation_data=<keras_pre..., epochs=1, validation_steps=10000)`\n",
      "  \"\"\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60000/60000 [==============================] - 2610s 44ms/step - loss: 0.0424 - acc: 0.9862 - val_loss: 0.9017 - val_acc: 0.8926\n"
     ]
    }
   ],
   "source": [
    "batches = datagen.flow(x_train, y_train, batch_size=64)\n",
    "val_batches=datagen.flow(x_test, y_test, batch_size=64)\n",
    "\n",
    "#fashion_model.compile(loss=keras.losses.categorical_crossentropy, optimizer=keras.optimizers.Adam(),metrics=['accuracy'])\n",
    "history=fashion_model.fit_generator(batches, batches.n, nb_epoch=1,validation_data=val_batches, nb_val_samples=val_batches.n)\n",
    "\n",
    "#fashion_model.fit_generator(x=x_train,y=y_train, epochs=10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MwQQW5iOJjwq"
   },
   "source": [
    "###  Report the final train and validation accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "c1SrtBEPJjwq"
   },
   "outputs": [],
   "source": [
    "# the first iteration with fit, we got close to 95 % accuracy. things did not improve much with dropout variable.25 this was \n",
    "#hovering around 98% .\n",
    "# but when fit_generator is used, the accuracy is 98.62% and it does not go beyond that.\n",
    "\n",
    "# the loss function  is quite mininal here .04 only\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZBwVWNQC2qZD"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8KXqmUDW2rM1"
   },
   "source": [
    "## **DATA AUGMENTATION ON CIFAR10 DATASET**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8mja6OgQ3L18"
   },
   "source": [
    "One of the best ways to improve the performance of a Deep Learning model is to add more data to the training set. Aside from gathering more instances from the wild that are representative of the distinction task, we want to develop a set of methods that enhance the data we already have. There are many ways to augment existing datasets and produce more robust models. In the image domain, these are done to utilize the full power of the convolutional neural network, which is able to capture translational invariance. This translational invariance is what makes image recognition such a difficult task in the first place. You want the dataset to be representative of the many different positions, angles, lightings, and miscellaneous distortions that are of interest to the vision task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6HzVTPUM3WZJ"
   },
   "source": [
    "### **Import neessary libraries for data augmentation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PPM558TX4KMb"
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import keras\n",
    "from keras.layers import Dense, Conv2D, BatchNormalization, Activation\n",
    "from keras.layers import AveragePooling2D, Input, Flatten\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import ModelCheckpoint, LearningRateScheduler\n",
    "from keras.callbacks import ReduceLROnPlateau\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.regularizers import l2\n",
    "from keras import backend as K\n",
    "from keras.models import Model\n",
    "from keras.datasets import cifar10\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "W6hicLwP4SqY"
   },
   "source": [
    "### **Load CIFAR10 dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NQ1WzrXd4WNk"
   },
   "outputs": [],
   "source": [
    "# Load the CIFAR10 data.\n",
    "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
    "# Normalize data.\n",
    "\n",
    "x_train = x_train.astype('float32') / 255\n",
    "x_test = x_test.astype('float32') / 255\n",
    "# Convert class vectors to binary class matrices.\n",
    "\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "\n",
    "y_test = keras.utils.to_categorical(y_test, num_classes)\n",
    "\n",
    "# Input image dimensions.\n",
    "input_shape = x_train.shape[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "R9Pht1ggHuiT"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 32, 32)\n"
     ]
    }
   ],
   "source": [
    "print (input_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3n28ccU6Hp6s"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JN3vYYhK4W0u"
   },
   "source": [
    "### **Create a data_gen funtion to genererator with image rotation,shifting image horizontally and vertically with random flip horizontally.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JJbekTKi4cmM"
   },
   "outputs": [],
   "source": [
    " datagen = ImageDataGenerator(\n",
    "\n",
    "        # set input mean to 0 over the dataset\n",
    "        featurewise_center=False,\n",
    "        # set each sample mean to 0\n",
    "        samplewise_center=False,\n",
    "        # divide inputs by std of dataset\n",
    "        featurewise_std_normalization=False,\n",
    "        # divide each input by its std\n",
    "        samplewise_std_normalization=False,\n",
    "        # apply ZCA whitening\n",
    "        zca_whitening=False,\n",
    "        # epsilon for ZCA whitening\n",
    "        zca_epsilon=1e-06,\n",
    "        # randomly rotate images in the range (deg 0 to 180)\n",
    "        rotation_range=0,\n",
    "        # randomly shift images horizontally\n",
    "        width_shift_range=0.1,\n",
    "        # randomly shift images vertically\n",
    "        height_shift_range=0.1,\n",
    "        # set range for random shear\n",
    "        shear_range=0.,\n",
    "        # set range for random zoom\n",
    "        zoom_range=0.,\n",
    "        # set range for random channel shifts\n",
    "        channel_shift_range=0.,\n",
    "        # set mode for filling points outside the input boundaries\n",
    "        fill_mode='nearest',\n",
    "        # value used for fill_mode = \"constant\"\n",
    "        cval=0.,\n",
    "        # randomly flip images\n",
    "        horizontal_flip=True,\n",
    "        # randomly flip images\n",
    "        vertical_flip=False,\n",
    "        # set rescaling factor (applied before any other transformation)\n",
    "        rescale=None,\n",
    "        # set function that will be applied on each input\n",
    "        preprocessing_function=None,\n",
    "        # image data format, either \"channels_first\" or \"channels_last\"\n",
    "        data_format=None,\n",
    "        # fraction of images reserved for validation (strictly between 0 and 1)\n",
    "        validation_split=0.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "e-SLtUhC4dK2"
   },
   "source": [
    "### **Prepare/fit the generator.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CSw8Bv2_4hb0"
   },
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy',\n",
    "\n",
    "              optimizer=Adam(lr=1e-3),\n",
    "\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "datagen.fit(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gYyF-P8O4jQ8"
   },
   "source": [
    "### **Generate 5 images for 1 of the image of CIFAR10 train dataset.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mXug4z234mwQ"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "R7_InternalLab_Questions.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
